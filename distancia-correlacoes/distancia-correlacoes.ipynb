{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Questionário: Distância e Correlações**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. QUESTÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média dos menores caminhos: 3.45\n",
      "Diâmetro: 14\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import networkx as nx\n",
    "\n",
    "# Carregando o arquivo de arestas\n",
    "with open(\"hamsterster.txt\", \"r\") as file:\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in file if line.strip()]\n",
    "\n",
    "# Criando o grafo\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Obtendo o maior componente conectado\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "# Calculando a média dos menores caminhos e o diâmetro\n",
    "average_shortest_path = nx.average_shortest_path_length(subgraph)\n",
    "diameter = nx.diameter(subgraph)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(f\"Média dos menores caminhos: {average_shortest_path:.2f}\")\n",
    "print(f\"Diâmetro: {diameter}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. QUESTÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média dos menores caminhos: 2.99\n",
      "Variância dos menores caminhos: 0.82\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "\n",
    "# Carregando o arquivo de arestas para a rede de aeroportos\n",
    "with open(\"USairport500.txt\", \"r\") as file:\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in file if line.strip()]\n",
    "\n",
    "# Criando o grafo\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Obtendo o maior componente conectado\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "# Calculando todos os menores caminhos entre pares\n",
    "lengths = dict(nx.all_pairs_shortest_path_length(subgraph))\n",
    "\n",
    "# Extraindo os comprimentos e armazenando numa lista\n",
    "all_lengths = []\n",
    "for source in lengths:\n",
    "    for target in lengths[source]:\n",
    "        if source != target:\n",
    "            all_lengths.append(lengths[source][target])\n",
    "\n",
    "# Calculando a média e variância\n",
    "mean_length = np.mean(all_lengths)\n",
    "var_length = np.var(all_lengths)\n",
    "\n",
    "# Exibindo os resultados\n",
    "print(f\"Média dos menores caminhos: {mean_length:.2f}\")\n",
    "print(f\"Variância dos menores caminhos: {var_length:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. QUESTÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de assortatividade: -0.07\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import networkx as nx\n",
    "\n",
    "# Carregando o arquivo de arestas para a rede Advogato\n",
    "with open(\"advogato.txt\", \"r\") as file:\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in file if line.strip()]\n",
    "\n",
    "# Criando o grafo dirigido (Advogato é uma rede dirigida)\n",
    "G = nx.DiGraph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Obtendo o maior componente fortemente conectado\n",
    "largest_scc = max(nx.strongly_connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_scc)\n",
    "\n",
    "# Calculando o coeficiente de assortatividade por grau\n",
    "assortativity = nx.degree_pearson_correlation_coefficient(subgraph)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(f\"Coeficiente de assortatividade: {assortativity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. QUESTÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropia de Shannon: 1.88\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import networkx as nx\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Carregando o arquivo de arestas para a rede USairport500\n",
    "with open(\"USairport500.txt\", \"r\") as file:\n",
    "    edges = [tuple(map(int, line.strip().split())) for line in file if line.strip()]\n",
    "\n",
    "# Criando o grafo\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Obtendo o maior componente conectado\n",
    "largest_cc = max(nx.connected_components(G), key=len)\n",
    "subgraph = G.subgraph(largest_cc)\n",
    "\n",
    "# Calculando todos os menores caminhos entre pares\n",
    "lengths = dict(nx.all_pairs_shortest_path_length(subgraph))\n",
    "\n",
    "# Extraindo os comprimentos e armazenando numa lista\n",
    "all_lengths = []\n",
    "for source in lengths:\n",
    "    for target in lengths[source]:\n",
    "        if source != target:\n",
    "            all_lengths.append(lengths[source][target])\n",
    "\n",
    "# Contando frequências e convertendo para distribuição de probabilidade\n",
    "counts = Counter(all_lengths)\n",
    "total = sum(counts.values())\n",
    "probs = [count / total for count in counts.values()]\n",
    "\n",
    "# Calculando entropia de Shannon com log base 2\n",
    "entropy = -sum(p * math.log2(p) for p in probs)\n",
    "\n",
    "# Exibindo o resultado\n",
    "print(f\"Entropia de Shannon: {entropy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. QUESTÃO**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coeficiente de correlação de Pearson: -0.16\n"
     ]
    }
   ],
   "source": [
    "# Importando as bibliotecas necessárias\n",
    "import networkx as nx\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# Carregando o arquivo de arestas\n",
    "# Certifique-se de que o arquivo \"word_adjacencies.txt\" exista no mesmo diretório\n",
    "try:\n",
    "    with open(\"word_adjacencies.txt\", \"r\") as file:\n",
    "        edges = [tuple(map(int, line.strip().split())) for line in file if line.strip()]\n",
    "except FileNotFoundError:\n",
    "    print(\"Erro: O arquivo word_adjacencies.txt não foi encontrado. Verifique o nome e o local do arquivo.\")\n",
    "    exit()\n",
    "\n",
    "# Criando o grafo\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "\n",
    "# Obtendo o maior componente conectado\n",
    "try:\n",
    "    largest_cc = max(nx.connected_components(G), key=len)\n",
    "    subgraph = G.subgraph(largest_cc).copy()\n",
    "except ValueError:  # Handle cases where the graph might be empty or disconnected\n",
    "    print(\"Erro: O grafo está vazio ou não possui componentes conectados.  Impossível calcular a correlação.\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Calculando o grau e o grau médio dos vizinhos\n",
    "degree_dict = dict(subgraph.degree())\n",
    "avg_neighbor_degree = nx.average_neighbor_degree(subgraph)\n",
    "\n",
    "# Coletando os valores para o cálculo da correlação\n",
    "graus = [degree_dict[node] for node in subgraph.nodes()]\n",
    "medias_vizinhos = [avg_neighbor_degree[node] for node in subgraph.nodes()]\n",
    "\n",
    "# Calculando o coeficiente de correlação de Pearson\n",
    "if len(graus) > 1 and len(medias_vizinhos) > 1: # Ensure there are enough data points for correlation\n",
    "    pearson_corr, _ = pearsonr(graus, medias_vizinhos)\n",
    "    print(f\"Coeficiente de correlação de Pearson: {pearson_corr:.2f}\")\n",
    "else:\n",
    "    print(\"Erro: Não é possível calcular a correlação.  O grafo pode ter apenas um nó no componente conectado ou estar vazio.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
